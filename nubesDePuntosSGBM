





import calib
import pickle
import numpy as np
import open3d as o3d
import cv2

# Cre Stereo

import os
import json
from pathlib import Path

from disparity.method_cre_stereo import CREStereo
from disparity.method_opencv_bm import StereoBM, StereoSGBM
from disparity.methods import Calibration, InputPair, Config

img=cv2.imread("./fotos_buddha_rectificadas/left_1_rect.jpg")

w, h = img.shape[1], img.shape[0]



with open("./data/calibracionBuddha/stereo_calibration_new.pkl", "rb") as f:
    c = pickle.load(f)

# Intrinsic matrices
K1 = c['left_K']
K2 = c['right_K']

# K = [[fx, 0, cx],
#      [0, fy, cy],
#      [0,  0,  1]]

fx  = K1[0, 0]
fy  = K1[1, 1]
cx0 = K1[0, 2]  # principal point x of left camera
cy0 = K1[1, 2]  # principal point y of left camera

# cx1 is from the rectified right projection matrix P2
# Compute rectification first:
R = c['R']
T = c['T']
image_size = c['image_size']
dist1 = c['left_dist']
dist2 = c['right_dist']

R1, R2, P1, P2, Q, _, _ = cv2.stereoRectify(
    K1, dist1,
    K2, dist2,
    image_size, R, T,
    flags=cv2.CALIB_ZERO_DISPARITY, alpha=0
)


if 'reprojection_error' in c:
    print(f"Calibration reprojection error: {c['reprojection_error']}")


K1_rect = P1[:3, :3]  # Use rectified camera matrix
K2_rect = P2[:3, :3]

cx1 = P2[0, 2]  # principal point x of rectified right camera



baseline = np.linalg.norm(T)
calibration = Calibration(**{
    "width": w,
    "height": h,
    "baseline_meters": baseline / 1000,
    "fx": fx,
    "fy": fy,
    "cx0": cx0,
    "cx1": cx0,
    "cy": cy0,
    "depth_range": [0.05, 20.0],
    "left_image_rect_normalized": [0, 0, 1, 1]
})

# copypaste del pdf consigna, seccion 5, usando el modulo calib de la tutorial 6

checkerboard=(10,7)
cuadradito_size_mm=24.2
object_3dpoints=calib.board_points(checkerboard)
object_3dpoints_mm=object_3dpoints*cuadradito_size_mm
nubes=[]

for i in range(0,6):      #tenemos 21 imagenes de banana
    left_rectified=cv2.imread(f"./fotos_buddha_rectificadas/left_{i}_rect.jpg")
    right_rectified=cv2.imread(f"./fotos_buddha_rectificadas/right_{i}_rect.jpg")
    left_rectified=cv2.cvtColor(left_rectified,cv2.COLOR_BGR2RGB)
    right_rectified=cv2.cvtColor(right_rectified,cv2.COLOR_BGR2RGB)

    # correct color conversion (OpenCV uses BGR)
    left_gray = cv2.cvtColor(left_rectified, cv2.COLOR_BGR2GRAY)
    right_gray = cv2.cvtColor(right_rectified, cv2.COLOR_BGR2GRAY)

    # detect boards and guard
    left_found, left_corners = calib.detect_board(checkerboard, left_gray)
    right_found, right_corners = calib.detect_board(checkerboard, right_gray)

    if not left_found or left_corners is None or len(left_corners) < 4:
        print(f"[{i}] left board not found or too few corners ({None if left_corners is None else len(left_corners)}) — skipping")
        continue
    if not right_found or right_corners is None or len(right_corners) < 4:
        print(f"[{i}] right board not found or too few corners ({None if right_corners is None else len(right_corners)}) — skipping")
        continue

    #imagen izquierda
    left_rectified_gray=cv2.cvtColor(left_rectified,cv2.COLOR_RGB2GRAY)
    left_rectified_gray = left_rectified_gray.astype(np.uint8)

    left_found,left_corners=calib.detect_board(checkerboard,left_rectified_gray)

    ret,rvec,tvec=cv2.solvePnP(object_3dpoints_mm,left_corners,K1_rect,None,flags=cv2.SOLVEPNP_IPPE)

    c_R_o_left = cv2.Rodrigues(rvec)[0]
    c_T_o_left = np.column_stack((c_R_o_left, tvec))
    c_T_o_left = np.vstack((c_T_o_left, [0, 0, 0, 1]))
    o_T_c_left = np.linalg.inv(c_T_o_left)        # object to camera transformation

    # DEBUG: Transform the checkerboard corners to world space
    # Reshape from (rows, cols, 3) to (N, 3)
    object_3dpoints_mm = object_3dpoints_mm.reshape(-1, 3)
    corners_3d_h = np.hstack([object_3dpoints_mm, np.ones((object_3dpoints_mm.shape[0], 1))])
    corners_world = (o_T_c_left @ corners_3d_h.T).T[:, :3]

    print(f"\n=== Image {i} ===")
    print(f"Checkerboard corners in world space:")
    print(f"X range: [{corners_world[:, 0].min():.2f}, {corners_world[:, 0].max():.2f}]")
    print(f"Y range: [{corners_world[:, 1].min():.2f}, {corners_world[:, 1].max():.2f}]")
    print(f"Z range: [{corners_world[:, 2].min():.2f}, {corners_world[:, 2].max():.2f}]")
    print(f"Center: [{corners_world[:, 0].mean():.2f}, {corners_world[:, 1].mean():.2f}, {corners_world[:, 2].mean():.2f}]")

    #imagen derecha
    right_rectified_gray=cv2.cvtColor(right_rectified,cv2.COLOR_RGB2GRAY)
    right_rectified_gray = right_rectified_gray.astype(np.uint8)

    # right_found,right_corners=calib.detect_board(checkerboard,right_rectified_gray)

    # print(f"Left corner Y: {left_corners[0, 0, 1]}")
    # print(f"Right corner Y: {right_corners[0, 0, 1]}")
    # print(f"Y difference: {abs(left_corners[0, 0, 1] - right_corners[0, 0, 1])}")




    # Create StereoSGBM matcher
    window_size = 5
    min_disp = 0
    num_disp = 16*8  # must be divisible by 16

    # stereo = cv2.StereoSGBM_create(
    #     minDisparity=min_disp,
    #     numDisparities=num_disp,
    #     blockSize=window_size,
    #     P1=8*3*window_size**2,
    #     P2=32*3*window_size**2,
    #     disp12MaxDiff=1,
    #     uniquenessRatio=10,
    #     speckleWindowSize=100,
    #     speckleRange=32
    # )

    stereo = cv2.StereoSGBM_create(
    minDisparity=0,
    numDisparities=16*10,  # Try larger range
    blockSize=5,
    P1=8*3*5**2,
    P2=32*3*5**2,
    disp12MaxDiff=1,
    uniquenessRatio=15,
    speckleWindowSize=150,
    speckleRange=2,
    mode=cv2.STEREO_SGBM_MODE_SGBM_3WAY
)

    disparity = stereo.compute(left_rectified_gray, right_rectified_gray).astype(np.float32) / 16

    # disparity_pixels = disparity.astype(np.float32) / 16.0       # now in pixels (float32)

    # # mark invalid values (optional)
    # disparity_pixels[disparity <= 0] = np.nan

    # print(type(disparity))
    # print(dir(disparity))

    # disp = disparity_pixels
    disp= disparity
    points_3D = cv2.reprojectImageTo3D(disp, Q)
    

    # Mask invalid points
    mask = disp > 0
    points = points_3D[mask]
    colors = left_rectified[mask] / 255.0  # normalize colors to [0, 1]

    scale_factor = 0.01  # Divide by 100 to match mm scale
    points = points * scale_factor

    # 4. Save disparity for inspection
    disp_vis = cv2.normalize(disparity, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)
    cv2.imwrite(f"debug_disparity_{i}.png", disp_vis)

    # pcd = o3d.geometry.PointCloud()
    # pcd.points = o3d.utility.Vector3dVector(points)
    # pcd.colors = o3d.utility.Vector3dVector(colors)
    # nubes.append(pcd)

    # transform points from left-camera coordinates to object/checkerboard (world) coordinates
    pts_h = np.hstack([points, np.ones((points.shape[0], 1), dtype=points.dtype)])  # (N,4)
    pts_world = (o_T_c_left @ pts_h.T).T[:, :3]  # use the camera->object 4x4 matrix you computed
    print(f"Baseline from T: {baseline} (units: probably mm)")
    print(f"Q matrix:\n{Q}")
    print(f"Q[3,2] (1/baseline): {Q[3,2]}")
    print(f"Expected baseline: {1/Q[3,2] if Q[3,2] != 0 else 'invalid'}")

    # After reconstruction, check point ranges
    print(f"Points range X: [{points[:, 0].min():.2f}, {points[:, 0].max():.2f}]")
    print(f"Points range Y: [{points[:, 1].min():.2f}, {points[:, 1].max():.2f}]")
    print(f"Points range Z: [{points[:, 2].min():.2f}, {points[:, 2].max():.2f}]")

# ##### para la bounding box
    # Define bounding box in world coordinates (mm)
    # Adjust these values based on your object's position
    x_min, x_max = 525,695  # X range
    y_min, y_max = 58,75   # Y range  
    z_min, z_max = -355,-345  # Z range (above checkerboard)

    # Filter points
    mask_box = (
        (pts_world[:, 0] >= x_min) & (pts_world[:, 0] <= x_max) &
        (pts_world[:, 1] >= y_min) & (pts_world[:, 1] <= y_max) &
        (pts_world[:, 2] >= z_min) & (pts_world[:, 2] <= z_max)
    )

    pts_world_filtered = pts_world[mask_box]
    colors_filtered = colors[mask_box]

    pcd = o3d.geometry.PointCloud()
    pcd.points = o3d.utility.Vector3dVector(pts_world_filtered)
    pcd.colors = o3d.utility.Vector3dVector(colors_filtered)
# #####
    # pcd = o3d.geometry.PointCloud()
    # pcd.points = o3d.utility.Vector3dVector(pts_world)
    # pcd.colors = o3d.utility.Vector3dVector(colors)
    # store tuple (pointcloud, camera->world transform) if you want frames later
    nubes.append((pcd, o_T_c_left))

# o3d.visualization.draw_geometries([pcd])
# Merge all transformed clouds into a single cloud
combined = o3d.geometry.PointCloud()

# frames = []
# # optional: board/world origin frame
# frame_board = o3d.geometry.TriangleMesh.create_coordinate_frame(size=50.0, origin=[0, 0, 0])
# frames.append(frame_board)

for idx, (pc, T) in enumerate(nubes):
    # pc is already in world coordinates (we transformed above), so just add
    combined += pc
#     # add a camera frame (camera pose expressed in world coordinates is inverse of o_T_c if needed)
#     cam_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=20.0)
#     # T is camera->world (o_T_c_left). If T maps differently, adjust accordingly.
#     cam_frame.transform(T)
#     frames.append(cam_frame)

o3d.visualization.draw_geometries([combined])
o3d.io.write_point_cloud("nubeDePuntosBuddha.ply", combined)




